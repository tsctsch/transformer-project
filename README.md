# Transformerä»é›¶å®ç°ä¸æ¶ˆèå®éªŒ

æœ¬é¡¹ç›®å®Œæ•´å®ç°äº†Transformeræ¨¡å‹ï¼Œå¹¶åœ¨IWSLT2017è‹±å¾·ç¿»è¯‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¶ˆèå®éªŒï¼ŒéªŒè¯äº†å„ä¸ªæ ¸å¿ƒç»„ä»¶çš„é‡è¦æ€§ã€‚

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### ä¸»è¦ç‰¹æ€§
- âœ… **å®Œæ•´Transformeræ¶æ„**ï¼šåŒ…å«Encoder-Decoderç»“æ„
- âœ… **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šæ”¯æŒå¤šå¤´æ³¨æ„åŠ›è®¡ç®—
- âœ… **ä½ç½®ç¼–ç **ï¼šæ­£å¼¦ä½ç½®ç¼–ç å®ç°
- âœ… **æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
- âœ… **æ¶ˆèå®éªŒ**ï¼šå¯¹æ¯”åˆ†æå„ç»„ä»¶é‡è¦æ€§
- âœ… **è®­ç»ƒå¯è§†åŒ–**ï¼šæŸå¤±æ›²çº¿å’Œå­¦ä¹ ç‡å˜åŒ–å¯è§†åŒ–

### å®ç°çš„æ ¸å¿ƒæ¨¡å—
- Multi-Head Self-Attention
- Position-wise Feed-Forward Network
- Residual Connections + Layer Normalization
- Positional Encoding (Sinusoidal)
- ç¼–ç å™¨-è§£ç å™¨æ¶æ„

## âš™ï¸ ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **æœ€ä½é…ç½®**: 8GB RAM, 10GB å­˜å‚¨ç©ºé—´
- **æ¨èé…ç½®**: 16GB RAM, GPU (â‰¥8GBæ˜¾å­˜)
- **æ”¯æŒè®¾å¤‡**: CPU/GPU (è‡ªåŠ¨æ£€æµ‹)

### è½¯ä»¶è¦æ±‚
- Python 3.8+
- PyTorch 1.9.0+
- å…¶ä»–ä¾èµ–è§requirements.txt

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨è¿è¡Œè„šæœ¬ï¼ˆæ¨èï¼‰
```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd transformer-project/src

# èµ‹äºˆæ‰§è¡Œæƒé™
chmod +x run.sh

# è¿è¡Œå®Œæ•´å®éªŒ
./run.sh
```

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨è¿è¡Œ
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd src

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œä¸»ç¨‹åº
python Trans.py
```

### ç²¾ç¡®é‡ç°å‘½ä»¤
```bash
cd src
export PYTHONHASHSEED=42
python Trans.py
```

## ğŸ“Š æ•°æ®é›†

æœ¬é¡¹ç›®ä½¿ç”¨ **IWSLT2017è‹±å¾·ç¿»è¯‘æ•°æ®é›†**ï¼ŒåŒ…å«çº¦200Kå¹³è¡Œå¥å¯¹ã€‚

### æ•°æ®é›†ä¿¡æ¯
- **ä»»åŠ¡ç±»å‹**: æœºå™¨ç¿»è¯‘ (EN â†” DE)
- **æ•°æ®è§„æ¨¡**: ~200,000 å¹³è¡Œå¥å¯¹
- **æ¥æº**: [Hugging Face Datasets - iwslt2017](https://huggingface.co/datasets/iwslt2017)

### æ•°æ®é¢„å¤„ç†
- æ–‡æœ¬å°å†™åŒ–
- è¯æ±‡è¡¨æ„å»º (min_freq=5)
- åºåˆ—é•¿åº¦é™åˆ¶ (max_seq_len=50)
- ç‰¹æ®Šæ ‡è®°: `<pad>`, `<sos>`, `<eos>`, `<unk>`

## ğŸ§  æ¨¡å‹æ¶æ„

### è¶…å‚æ•°é…ç½®
| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| åµŒå…¥ç»´åº¦ | 256 | è¯å‘é‡ç»´åº¦ |
| æ³¨æ„åŠ›å¤´æ•° | 4 | å¤šå¤´æ³¨æ„åŠ›å¤´æ•° |
| ç¼–ç å™¨å±‚æ•° | 2 | Transformerå±‚æ•° |
| è§£ç å™¨å±‚æ•° | 2 | Transformerå±‚æ•° |
| å‰é¦ˆç½‘ç»œç»´åº¦ | 512 | FFNéšè—å±‚ç»´åº¦ |
| æ‰¹å¤§å° | 16 | è®­ç»ƒæ‰¹å¤§å° |
| å­¦ä¹ ç‡ | 1e-4 | åˆå§‹å­¦ä¹ ç‡ |
| Dropout | 0.1 | é˜²æ­¢è¿‡æ‹Ÿåˆ |


### æ ¸å¿ƒå…¬å¼

#### Scaled Dot-Product Attention
```
Attention(Q,K,V) = softmax(QKáµ€/âˆšdâ‚–)V
```

#### Multi-Head Attention
```
MultiHead(Q,K,V) = Concat(headâ‚,...,headâ‚•)Wá´¼
```
å…¶ä¸­ï¼š
```
headáµ¢ = Attention(QWáµ¢áµ , KWáµ¢á´·, VWáµ¢â±½)
```

#### Positional Encoding
```
PE(pos,2i) = sin(pos / 10000^(2i/d_model))
PE(pos,2i+1) = cos(pos / 10000^(2i/d_model))
```

#### å±‚å½’ä¸€åŒ–
```
LayerNorm(x) = Î³ â‹… (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

#### æ®‹å·®è¿æ¥
```
Output = LayerNorm(x + Sublayer(x))
```

#### ä½ç½®å‰é¦ˆç½‘ç»œ
```
FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚
```



## ğŸ”¬ æ¶ˆèå®éªŒ

æœ¬é¡¹ç›®åŒ…å«4ç§æ¨¡å‹å˜ä½“çš„æ¶ˆèå®éªŒï¼š

1. **å®Œæ•´æ¨¡å‹** (full): æ ‡å‡†Transformeræ¶æ„
2. **æ— ä½ç½®ç¼–ç ** (no_positional): ç§»é™¤ä½ç½®ç¼–ç 
3. **å•å¤´æ³¨æ„åŠ›** (single_head): ä½¿ç”¨å•å¤´è€Œéå¤šå¤´æ³¨æ„åŠ›
4. **æ— æ®‹å·®è¿æ¥** (no_residual): ç§»é™¤æ®‹å·®è¿æ¥

### å®éªŒè®¾ç½®
- **éšæœºç§å­**: 42
- **è®­ç»ƒè½®æ•°**: 5
- **è¯„ä¼°æŒ‡æ ‡**: äº¤å‰ç†µæŸå¤±
- **ä¼˜åŒ–å™¨**: Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=1e-9)
- **å­¦ä¹ ç‡è°ƒåº¦**: StepLR (step_size=2, gamma=0.8)

## ğŸ“ˆ å®éªŒç»“æœ

å®éªŒå°†ç”Ÿæˆä»¥ä¸‹ç»“æœæ–‡ä»¶ï¼š

### å¯è§†åŒ–å›¾è¡¨
- `ablation_results_table.png`: æ¶ˆèå®éªŒç»“æœæ±‡æ€»è¡¨æ ¼
- `ablation_comparison.png`: æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
- `all_training_curves.png`: æ‰€æœ‰æ¨¡å‹è®­ç»ƒæ›²çº¿å¯¹æ¯”
- `*_training_curves.png`: å•ä¸ªæ¨¡å‹çš„è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿

### æ•°æ®æ–‡ä»¶
- `detailed_results.json`: è¯¦ç»†çš„å®éªŒç»“æœæ•°æ®
- `config.json`: å®éªŒé…ç½®å‚æ•°
- `*_best_model.pth`: å„æ¨¡å‹çš„æœ€ä½³æƒé‡æ–‡ä»¶
- `progress.json`: è®­ç»ƒè¿‡ç¨‹è®°å½•

## ğŸ’¡ å…³é”®å®ç°

### æ ¸å¿ƒä»£ç ç‰‡æ®µ

```python
# å¤šå¤´æ³¨æ„åŠ›å®ç°
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, num_heads, 
                                              dropout=dropout, batch_first=True)
    
    def forward(self, x, key_padding_mask=None):
        attn_out, _ = self.self_attn(x, x, x, 
                                   key_padding_mask=key_padding_mask)
        return attn_out
```

```python
# ä½ç½®ç¼–ç å®ç°
class PositionalEncoding(nn.Module):
    def __init__(self, d_model=256, max_len=100):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))
```

## ğŸ› ï¸ è®­ç»ƒç‰¹æ€§

### è®­ç»ƒç¨³å®šæ€§æŠ€å·§
- **æ¢¯åº¦è£å‰ª**: `max_norm=1.0`
- **æ··åˆç²¾åº¦è®­ç»ƒ**: è‡ªåŠ¨æ£€æµ‹GPUå¹¶å¯ç”¨
- **å­¦ä¹ ç‡è°ƒåº¦**: StepLRåŠ¨æ€è°ƒæ•´
- **æ—©åœæœºåˆ¶**: åŸºäºéªŒè¯æŸå¤±ä¿å­˜æœ€ä½³æ¨¡å‹

### å†…å­˜ä¼˜åŒ–
- è‡ªåŠ¨GPUå†…å­˜ç®¡ç†
- ç©ºæ‰¹æ¬¡è·³è¿‡
- æ··åˆç²¾åº¦è®­ç»ƒå‡å°‘æ˜¾å­˜å ç”¨

## ğŸ“ è¿è¡Œè¯´æ˜

### é¢„æœŸè¿è¡Œæ—¶é—´
- **CPU**: ~30-60åˆ†é’Ÿ
- **GPU**: ~10-20åˆ†é’Ÿ

### è¾“å‡ºè¯´æ˜
è¿è¡Œå®Œæˆåï¼Œå°†åœ¨`../results/`ç›®å½•ä¸‹ç”Ÿæˆï¼š
- æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
- JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
- è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡
- å®éªŒé…ç½®å¤‡ä»½

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ•°æ®é›†ä¸å­˜åœ¨**
   ```bash
   # æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†
   python -c "from datasets import load_dataset; load_dataset('iwslt2017', 'iwslt2017-de-en')"
   ```

2. **GPUå†…å­˜ä¸è¶³**
   - è‡ªåŠ¨å‡å°‘æ‰¹å¤§å°
   - å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
   - æ¸…ç†GPUç¼“å­˜

3. **ä¾èµ–å®‰è£…å¤±è´¥**
   ```bash
   # ä½¿ç”¨condaå®‰è£…PyTorch
   conda install pytorch torchvision torchaudio pytorch-cuda -c pytorch -c nvidia
   pip install -r requirements.txt
   ```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦æœ¯ç ”ç©¶ç›®çš„ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ [Attention Is All You Need](https://arxiv.org/abs/1706.03762) è®ºæ–‡ä½œè€…
- ä½¿ç”¨ [IWSLT2017](https://huggingface.co/datasets/iwslt2017) æ•°æ®é›†
- åŸºäº PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶

## ğŸ“§ è”ç³»ä¿¡æ¯

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- é‚®ç®±: [your-email@example.com]
- GitHub: [your-username]

---

*æœ€åæ›´æ–°: 2025å¹´11æœˆ*
```